title: Service Instance Scaling

tags: contrail

description: |
    Service Instance Scaling

    This test validates scaling of Contrail service instances within an OS::Heat::AutoScalingGroup.

    Special things to note:

    1.
    The master VM needs to be able to reach the alarm URL generated by Heat to trigger the scaling.
    This is on the OpenStack endpoint associated with cloudformation service type (e.g. heat-cfn).
    Typically Ceilometer or some other resource monitoring tool would call this URL, but we are
    triggering the alarm manually.

    2.
    The resources within the autoscaling group are contained in a separate template, which by
    necessity must be hosted on a web server that is reachable by the heat-engine (i.e. mosc).

    Currently the nested template is retrieved directly from gerrit.
    https://gerrit.mtn5.cci.att.com/gitweb?p=aic-aqa-resource.git;a=blob_plain;f=aic_tests/contrail/shaker/templates/service_instance_autoscale_depends.yaml

    3.
    AutoScaling with Contrail resources does not work in AIC 3.0.3 (as of Contrail 3.2.11) without
    the following patch:

    diff -r -u a/contrail_heat/resources/contrail.py b/contrail_heat/resources/contrail.py
    --- a/contrail_heat/resources/contrail.py	2018-06-12 09:22:46.000000000 +0000
    +++ b/contrail_heat/resources/contrail.py	2018-08-11 05:33:55.310396459 +0000
    @@ -16,7 +16,12 @@

     def set_auth_token(func):
         def wrapper(self, *args, **kwargs):
    -        self.vnc_lib().set_auth_token(self.stack.context.auth_token)
    +        token = None
    +        if self.stack.context.auth_token:
    +            token = self.stack.context.auth_token
    +        else:
    +            token = self.vnc_lib().get_auth_token()
    +        self.vnc_lib().set_auth_token(token)
             return func(self, *args, **kwargs)
         return wrapper

    4.
    Scaling down is fast, but the amount of time it takes to scale is somewhat unpredictable, which
    means that we either need to sleep for a long time before running the bandwidth test, or try to
    do something else to detect when the scaling is done.

    The basic idea of this optimization is to leverage the fact that the service instances are doing
    NAT. We can make queries from master to slave, and keep track of the number of unique source IPs
    seen over a span of queries. Each unique source would represent the right-side network IP of the
    service VM.

    We have a small http server on the slave (like ipchicken.com) that returns the IP address of the
    client. After each scaling trigger, the master VM makes a bunch of queries to the slave in this
    manner, until a change in the number of service instances occurs.

    Scenario Diagram
                                                      +--------------------+
    +----------+                                      |                    |            +----------+
    |          |                                      |                    |            |          |
    |  master  |+------------------------------------>|  service instance  |+---------->|  slave   |
    |          |                                +++   |                    |   ^^^      |          |
    +----------+                                |||   |                    |   |||      +----------+
                                                |||   +--------------------+   |||
    1. iperf sends traffic from master to slave |||                            |||
    (using multiple threads)                    ||| +------------------------+ |||
                                                ||| |   auto scaling group   | |||
    network policy sends traffic through the    ||| +------------------------+ |||
    service instance and traffic is distributed ||| |                        | |||
    among service vms. (service vms are linked  ||| |    +--------------+    | |||
    to the service instance via port tuples)    ||| |    |              |    | |||
                                                ||+----->|  service vm  |+-----+||
    throughput measurement confirms the number  ||  |    |              |*   |  ||
    of active service vms (initially 2)         ||  |    +--------------+    |  ||
                                                ||  |                        |  ||
    2. master vm triggers scaling up by calling ||  |    +--------------+    |  ||
    alarm url directly (now 3 svms)             ||  |    |              |    |  ||
                                                |+------>|  service vm  |+------+|
    3. iperf (avg throughput ~270Mbps)          |   |    |              |*   |   |
                                                |   |    +--------------+    |   |
    4. scale down (to 2 svms)                   |   |                        |   |
                                                |   |          ...           |   |
    5. iperf (avg throughput ~180Mbps)          |   |                        |   |
                                                |   |    +--------------+    |   |
    6. scale down (to 1 svm)                    |   |    |              |    |   |
                                                +------->|  service vm  |+-------+
    7. iperf (avg throughput ~90Mbps)               |    |              |*   |
                                                    |    +--------------+    |  *output rate on
    8. scale up (back to 2 svms)                    |                        |  each service vm
                                                    +------------------------+  limited to
    9. iperf (avg throughput ~180Mbps)                                          100Mbps

deployment:
  template: templates/service_instance_autoscale.yaml
  env_file: env/service_instance_autoscale.env
  accommodation: [pair, compute_nodes: 1]

execution:
  tests:
    -
      title: 1. Bandwidth test with two service instances
      class: iperf3
      threads: 10
      time: 30
      sla:
      - "[type == 'agent'] >> ((stats.bandwidth.avg > 160) and (stats.bandwidth.avg < 200))"
    -
      title: 2. Trigger Scale Up
      class: shell
      script: curl -sSfk -m 10 -X POST "$(cat /opt/ctd/scaleup_url.txt)"
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 2b. Wait for scaling
      class: shell
      script: /opt/ctd/wait-for-scaling-or-timeout.sh 300 3
      time: 300
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 3. Bandwidth test after scale up to three service instances
      class: iperf3
      threads: 10
      time: 30
      sla:
      - "[type == 'agent'] >> ((stats.bandwidth.avg > 240) and (stats.bandwidth.avg < 300))"
    -
      title: 4. Trigger Scale Down
      class: shell
      script: curl -sSfk -m 10 -X POST "$(cat /opt/ctd/scaledown_url.txt)"
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 4b. Wait for scaling
      class: shell
      script: /opt/ctd/wait-for-scaling-or-timeout.sh 60 2
      time: 60
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 5. Bandwidth test after scale down to two service instances
      class: iperf3
      threads: 10
      time: 30
      sla:
      - "[type == 'agent'] >> ((stats.bandwidth.avg > 160) and (stats.bandwidth.avg < 200))"
    -
      title: 6. Trigger Scale Down
      class: shell
      script: curl -sSfk -m 10 -X POST "$(cat /opt/ctd/scaledown_url.txt)"
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 6b. Wait for scaling
      class: shell
      script: /opt/ctd/wait-for-scaling-or-timeout.sh 60 1
      time: 60
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 7. Bandwidth test after scale down to one service instances
      class: iperf3
      threads: 10
      time: 30
      sla:
      - "[type == 'agent'] >> ((stats.bandwidth.avg > 80) and (stats.bandwidth.avg < 100))"
    -
      title: 8. Trigger Scale Up
      class: shell
      script: curl -sSfk -m 10 -X POST "$(cat /opt/ctd/scaleup_url.txt)"
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 8b. Wait for scaling
      class: shell
      script: /opt/ctd/wait-for-scaling-or-timeout.sh 300 2
      time: 300
      sla:
      - "[type == 'agent'] >> (stderr == '')"
    -
      title: 9. Bandwidth test after scale up to two service instances
      class: iperf3
      threads: 10
      time: 30
      sla:
      - "[type == 'agent'] >> ((stats.bandwidth.avg > 160) and (stats.bandwidth.avg < 200))"
